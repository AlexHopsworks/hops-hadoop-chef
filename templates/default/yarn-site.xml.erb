<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>


<!-- --------------------- MARC --------------------------------------------------------- -->


	<property>
		<name>yarn.resourcemanager.resource-tracker.address</name>
		<value>hdfs://<%= @rm_ip %>:8031</value>
		<description>host is the hostname of the resource manager and port is the port on which the NodeManagers contact the Resource Manager.</description>
	</property>
	<property>
		<name>yarn.resourcemanager.scheduler.address</name>
		<value>hdfs://<%= @rm_ip %>:8030</value>
	</property>
	<property>
		<name>yarn.resourcemanager.address</name>
		<value>hdfs://<%= @rm_ip %>:8032</value>
	</property>
	<property>
		<name>yarn.resourcemanager.admin.address</name>
		<value>hdfs://<%= @rm_ip %>:8033</value>
	</property>
	<property>
		<name>yarn.web-proxy.address</name>
		<value>hdfs://<%= @rm_ip %>:<%= node[:hadoop][:yarn][:ps_port] %></value>
	</property>
	<property>
		<name>yarn.application.classpath</name>
		<value><%= node[:hadoop][:yarn][:app_classpath] %></value>
	</property>
	<property>
		<name>yarn.resourcemanager.scheduler.class</name>
<!--		<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value> -->
		<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler</value>
		<description>In case you do not want to use the default scheduler</description>
	</property>
	<property>
		<name>yarn.nodemanager.local-dirs</name>
		<value><%= node[:hadoop][:tmp_dir] %>/nm-local-dir</value>
		<description>the local directories used by the nodemanager to store its localized files</description>
	</property>
	<property>
		<name>yarn.nodemanager.log-dirs</name>
		<value></value>
		<description>the local directories used by the nodemanager to store its localized logs</description>
	</property>
	<property>
		<name>yarn.nodemanager.remote-app-log-dir</name>
		<value></value>
		<description>the directory where the logs are aggregated</description>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value><%= node[:hadoop][:yarn][:aux_services] %></value>
		<description>shuffle service that needs to be set for Map Reduce to run</description>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
		<value><%= node[:hadoop][:mr][:shuffle_class] %></value>
		<description>The exact name of the class for shuffle service</description>
	</property>
	<property>
		<name>yarn.nodemanager.vmem-pmem-ratio</name>
		<value><%= node[:hadoop][:yarn][:vpmem_ratio] %></value>
		<description>The virtual memory (physical + paged memory) upper limit for each Map and Reduce task is determined by the virtual memory ratio each YARN Container is allowed.</description>
	</property>
	<property>
		<name>yarn.nodemanager.resource.memory-mb</name>
		<value><%= @available_mem_mb %></value>
		<description>the amount of memory on the NodeManager in MB</description>
	</property>
	<property>
		<name>yarn.scheduler.minimum-allocation-mb</name>
		<value>128</value>
	</property>
	<property>
		<name>yarn.nodemanager.resource.cpu-vcores</name>
		<value><%= node[:hadoop][:yarn][:vcores] %></value>
		<description>Number of CPU cores that can be allocated for containers.</description>
	</property>
	<property>
		<name>yarn.scheduler.minimum-allocation-vcores</name>
		<value><%= node[:hadoop][:yarn][:min_vcores] %></value>
		<description>The minimum allocation for every container request at the RM, in terms of virtual CPU cores. Requests lower than this won't take effect, and the specified value will get allocated the minimum.</description>
	</property>
	<property>
		<name>yarn.scheduler.maximum-allocation-vcores</name>
		<value><%= node[:hadoop][:yarn][:max_vcores] %></value>
		<description>The maximum allocation for every container request at the RM, in terms of virtual CPU cores. Requests higher than this won't take effect, and will get capped to this value.</description>
	</property>
	<property>
		<name>yarn.nodemanager.log.retain-seconds</name>
		<value><%= node[:hadoop][:yarn][:log_retain_secs] %></value>
		<description>Default time (in seconds) to retain log files on the NodeManager Only applicable if log-aggregation is disabled.</description>
	</property>
	<property>
		<name>yarn.resourcemanager.am.max-retries</name>
		<value><%= node[:hadoop][:am][:max_retries] %></value>
		<description>Number of times to try to restart the ApplicationMaster by the ResourceManager if its NodeManager fails.</description>
	</property>


<!--
  <property>
    <name>yarn.nodemanager.address</name>
    <value><%= @my_ip %>:50060</value>
    <value><%= node['fqdn'] %>:50060</value>
    <description>the nodemanagers bind to this port</description>
  </property>  

  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>hdfs://<%= @rm_ip %>:8088</value>
    <description>This is currently causing the RM to crash.</description>
  </property>

  <property>
    <name>yarn.resourcemanager.webapp.https.address</name>
    <value>hdfs://<%= @rm_ip %>:8090</value>
  </property>

   <property>
     <name>yarn.nodemanager.container-executor.class</name>
     <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
     <description>who will execute(launch) the containers.</description>
   </property>

   <property>
     <name>yarn.nodemanager.linux-container-executor.resources-handler.class</name>
     <value>org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler</value>
     <description>The class which should help the LCE handle resources.</description>
   </property>

   <property>
     <name>yarn.nodemanager.linux-container-executor.cgroups.hierarchy</name>
     <value>/hadoop-yarn</value>
     <description>The cgroups hierarchy under which to place YARN proccesses (cannot contain commas).
     If yarn.nodemanager.linux-container-executor.cgroups.mount is false (that is, if cgroups have
     been pre-configured), then this cgroups hierarchy must already exist and be writable by the
     NodeManager user, otherwise the NodeManager may fail.
     Only used when the LCE resources handler is set to the CgroupsLCEResourcesHandler.</description>
   </property>

   <property>
     <name>yarn.nodemanager.linux-container-executor.cgroups.mount</name>
     <value>true</value>
     <description>Whether the LCE should attempt to mount cgroups if not found.
     Only used when the LCE resources handler is set to the CgroupsLCEResourcesHandler.</description>
   </property>

   <property>
     <name>yarn.nodemanager.linux-container-executor.cgroups.mount-path</name>
     <value>/cgroup</value>
     <description>Where the LCE should attempt to mount cgroups if not found. Common locations
     include /sys/fs/cgroup and /cgroup; the default location can vary depending on the Linux
     distribution in use. This path must exist before the NodeManager is launched.
     Only used when the LCE resources handler is set to the CgroupsLCEResourcesHandler, and
     yarn.nodemanager.linux-container-executor.cgroups.mount is true.</description>
   </property>

   <property>
     <name>yarn.nodemanager.linux-container-executor.group</name>
     <value><%= node[:hadoop][:yarn] %></value>
   </property>

-->

</configuration>
